{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7cf205-9ad2-459e-80de-a2fc37284285",
   "metadata": {},
   "source": [
    "**SDPA Final Coursework, Part 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d01a3cc0-a36d-474a-9875-a5016cde901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import datetime\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import sys\n",
    "import re\n",
    "import yfinance as yf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17cefd1-6b8e-41cd-94d1-2bd8f4079558",
   "metadata": {},
   "source": [
    "**Step 1: Crawl a real-world dataset**\n",
    "\n",
    "The data of focus for this section of the coursework will be tweet data from the social media platform and digital town square, Twitter. Initially, the plan for the project was to get direct access to the twitter API by applying for an educational account and use the API keys generated by the account to access tweets of my defined specification, but in the end, I was not able to get an account. \n",
    "\n",
    "Instead, I have implemented the external library 'snscraper' to retrieve tweets of my defined specification. It works analogous to; and parameters are largely defined similarly to the native Twitter documentation, but there is no need for API keys, which has allowed more effort to be put into development, as opposed to API bureaucracy. More information based on ‘snscraper’ and all other external libraries involved in the project can be found in the README.md. file for Part 2 of the assignment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c3f23d-4528-46f5-bb4d-92cbb488cb51",
   "metadata": {},
   "source": [
    "**Step 2a: Perform data preparation & cleaning**\n",
    "\n",
    "The only section of data that needs cleaning and formatting is the tweet text itself, all other parameters scraped by specification, like date, tweet ID, reply count, retweet count and like count have been scraped to a proper format. In order to remove unnecessary parts and characters from tweet text, a list of regular expressions will be applied to the scraped tweets in an iterative fashion. The pre-defined function below is called and applied to the scraping and enriching in later steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1f1211c1-33d4-4171-a4c6-637ccfcaee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # Removing @mentions. \n",
    "    text = re.sub('@[A-Za-z0–9]+', '', text) \n",
    "    \n",
    "    # Removing '#' hash tags. \n",
    "    text = re.sub('#', '', text) \n",
    "    \n",
    "    # Removing RTs. \n",
    "    text = re.sub('RT[\\s]+', '', text) \n",
    "    \n",
    "     # Removing hyperlinks. \n",
    "    text = re.sub('https?:\\/\\/\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation. \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    \n",
    "    # Remove underscores. \n",
    "    text = re.sub(r'_', '', text) \n",
    "    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1efdb51-92ef-4190-96b9-61b780105b11",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Step 2b: Generating daily (numerical) means of existing columns & enriching data by adding sentiment, subjectivity columns and price data of specified asset **\n",
    "\n",
    "As per the specification of the scraping process, 100 tweets of the given keyword 'bitcoin' have been scraped for the past 150 as of 24/11/2022, this totalling into a raw dataset of 15,000 rows of tweet text data and other auxiliary information stored in other columns over the course of the defined period. Each of the tweets are not so useful in and of themselves. For example, the value of a numeric column (e.g, like count, retweet count) on a particular day is not very useful, that given tweet might have been particularly popular too; it's not a fair reflection of the overall day. To get a true impression of the average of a numeric figure (e.g, like count, retweet count) on a particular day, it would be far more insightful to get a mean of each of these variables over 100 tweets for each day. Therefore, a mean of each numeric value has replaced the 100 values for daily tweets, in the aim of providing a fairer reflection of the general popularity / performance of each tweet. \n",
    "\n",
    "The fact that tweet data is text-based data, also means there are potentially a great number of insights to be found in the generation of sentiment and subjectivity figures using sentiment analysis. Sentiment and subjectivity scores allow us to interpret tweets; or groups of tweets, and understand the intention behind them, in doing so, we can form an idea of what the public things of a particular thing, in this case the asset 'bitcoin'. The sentiment analyser VADER has been applied to all tweets, an average calculated for all tweets per day (100) and appended to final dataset as a form of enrichment. The same has been done for subjectivity scores. More information on the library 'nltk' and the sentiment analysis tool 'VADER', and all other external libraries involved in the project can be found in the README.md. file for Part 2 of the assignment. \n",
    "\n",
    "Finally, BTC data from Yahoo! Finance has also been scraped by calling their API to make potentially interesting comparisons between public sentiment and asset price. More information on the ‘yfinance’ library and all other external libraries involved in the project can be found in the README.md. file for Part 2 of the assignment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "df15c76c-5da3-43f7-bdc3-52ab43ecd427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_scrape_enrich_aggregate():\n",
    "    import datetime\n",
    "    # List of dates tweets are retrieved from. \n",
    "    dates = []\n",
    "\n",
    "\n",
    "    mean_reply_count_list = []\n",
    "    mean_retweet_count_list = []\n",
    "    mean_like_count_list = []\n",
    "\n",
    "    # List for mean sentiment and mean subjectivity to be appended to later. \n",
    "    mean_sentiment_list = []\n",
    "    mean_subjectivity_list = []\n",
    "\n",
    "    # List of all data scraped from Twitter API. \n",
    "    whole_data = []\n",
    "\n",
    "\n",
    "    # Inputs for key parameters, for this project they have been inputted as the following: bitcoin, 100, 150 . \n",
    "    # This is here in case other keywords need to be tested, but static variable are fine too. \n",
    "    keyword = (input(\"Please enter a keyword or phrase to focus your search: \"))\n",
    "    NoOfTweets = int(input(\"Please enter the number of Tweets you would like to analyse per day: \"))\n",
    "    days = int(input(\"How many of the last days would you like to analyse?:  \"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Loop through the last n days, 150 days for this project. \n",
    "    for i in range(days):\n",
    "      # Calculate the date for the current iteration\n",
    "        start = datetime.datetime.now() - datetime.timedelta(days=i)\n",
    "        end = datetime.datetime.now() - datetime.timedelta(days=(i-1 ))\n",
    "        date_str = start.strftime('%Y-%m-%d')\n",
    "\n",
    "      # Add the date for current iteration to the list of dates. \n",
    "        dates.append(date_str)\n",
    "\n",
    "    # Reverse list of days to get items in chronological order.      \n",
    "    dates = dates[::-1]\n",
    "\n",
    "\n",
    "    # First for loop iterating through list of dates previously generated. \n",
    "    for date in dates:\n",
    "        sentiment_scores = []\n",
    "        subjectivity_scores = []\n",
    "\n",
    "        # Empty lists for data of each 100 tweets from each day to be appended to. \n",
    "        tweet_list = []\n",
    "        like_count_list = []\n",
    "        reply_count_list = []\n",
    "        retweet_count_list = []\n",
    "\n",
    "        # test\n",
    "        # print(date) \n",
    "\n",
    "        # Second for loop scraping for 100 tweets iteratively for the date of the outer for loop iteration. \n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{keyword} lang:en until:{date}').get_items()):\n",
    "            if i>=NoOfTweets: \n",
    "                break \n",
    "            # Appending all raw data from tweet.     \n",
    "            whole_data.append([tweet.date, tweet.id, tweet.rawContent, tweet.user.username, tweet.replyCount, tweet.retweetCount, tweet.likeCount])\n",
    "\n",
    "            # Adding specific data to list of particular description, to be made into mean lists later. \n",
    "            tweet_list.append(tweet.rawContent)\n",
    "            like_count_list.append(tweet.likeCount)\n",
    "            reply_count_list.append(tweet.replyCount)\n",
    "            retweet_count_list.append(tweet.retweetCount)\n",
    "            \n",
    "            \n",
    "        for tweet in tweet_list:\n",
    "            \n",
    "            # Apply tweet cleaner to tweets before sentiment analysis for more reliable results. \n",
    "            tweet = clean_text(tweet)\n",
    "\n",
    "            analyser = SentimentIntensityAnalyzer()\n",
    "            # compound sentiment score from the specific tweet stored in the 'scores' variable using VADER. \n",
    "            scores = analyser.polarity_scores(tweet)\n",
    "            \n",
    "            # TextBlob object created for tweet iteratively. \n",
    "            analysis = TextBlob(tweet)\n",
    "            \n",
    "            # Compound sentiment and subjectivity scores append to related lists. \n",
    "            sentiment_scores.append(scores['compound'])\n",
    "            subjectivity_scores.append(analysis.sentiment.subjectivity)\n",
    "        \n",
    "        # test\n",
    "        # print(sentiment_scores)\n",
    "        \n",
    "\n",
    "            # Calculate the mean of numerical columns, mean sentiment and subjectivity scores for the day. \n",
    "        mean_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
    "        mean_sentiment_list.append(mean_sentiment)\n",
    "\n",
    "        mean_subjectivity = sum(subjectivity_scores) / len(subjectivity_scores)\n",
    "        mean_subjectivity_list.append(mean_subjectivity)\n",
    "\n",
    "        mean_like_count = sum(like_count_list) / len(like_count_list)\n",
    "        mean_like_count_list.append(mean_like_count)\n",
    "\n",
    "        mean_reply_count = sum(reply_count_list) / len(reply_count_list)\n",
    "        mean_reply_count_list.append(mean_reply_count)\n",
    "\n",
    "        mean_retweet_count = sum(retweet_count_list) / len(retweet_count_list)\n",
    "        mean_retweet_count_list.append(mean_retweet_count)\n",
    "\n",
    "            \n",
    "    print('Scraping and sentiment analysis complete!')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    keyword_list = [f'{keyword}']*days\n",
    "\n",
    "    # Add the mean sentiment, subjectivity, and price information to the data frame for bitcoin\n",
    "    df1 = pd.Series(keyword_list, name = 'Keyword')\n",
    "    df2 = pd.Series(dates, name = 'Date')\n",
    "    df3 = pd.Series(mean_reply_count_list, name = 'Mean reply count')\n",
    "    df4 = pd.Series(mean_retweet_count_list, name = 'Mean retweet count')\n",
    "    df5 = pd.Series(mean_like_count_list, name = 'Mean like count')\n",
    "    df6 = pd.Series(mean_sentiment_list, name = 'Mean sentiment of daily tweets')\n",
    "    df7 = pd.Series(mean_subjectivity_list, name = 'Mean subjectivity of daily tweets')\n",
    "    df8 = pd.Series(market_data(days), name = 'Adjusted closing asset price')\n",
    "    \n",
    "\n",
    "    df_bitcoin = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8], axis = 1)\n",
    "    print(f'df_{keyword_list[0]} created!')\n",
    "    df_bitcoin.to_csv(f'{keyword}_enriched_data.csv', index = False)\n",
    "    \n",
    "    \n",
    "#     # Aggregate raw data for initial csv file. \n",
    "#     df = pd.DataFrame(whole_data)\n",
    "#     df.columns = ['Date', 'ID', 'Text', 'Username', 'Reply count', 'Retweet count', 'Like count']\n",
    "#     df.to_csv(f'{keyword}_raw_extracted_data.csv', index = False)\n",
    "    \n",
    "    \n",
    "#     # API sracping Yahoo Finance for BTC price data within defined time range\n",
    "#     # Set necessary variables \n",
    "#     start = datetime.datetime(2022, 7, 29)\n",
    "#     end = datetime.datetime(2022, 12, 25)\n",
    "#     symbol = 'BTC-GBP'\n",
    "\n",
    "#     # Download specific data from Yahoo! Finance to a dataframe, a 6x150 dataframe. \n",
    "#     df= yf.download(symbol, start=start, end=end)\n",
    "\n",
    "#     df.to_csv(f'bitcoin_market_data_29722_251222.csv', index = False)\n",
    "\n",
    "    \n",
    "    return df_bitcoin\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "97fc79cc-4d1f-4f7e-b156-6d9f9ad664be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def market_data(days): \n",
    "    \n",
    "    # Set necessary variables \n",
    "    start = datetime.datetime(2022, 7, 29)\n",
    "    end = datetime.datetime(2022, 12, 25)\n",
    "    symbol = 'BTC-GBP'\n",
    "    \n",
    "    # Download specific data from Yahoo! Finance to a dataframe, a 6x150 dataframe. \n",
    "    df= yf.download(symbol, start=start, end=end)\n",
    "    \n",
    "    # Isolate particular column for adjusted market price at close. \n",
    "    df = df['Adj Close'][0:days]\n",
    "    # Change df to a list, so it can be concatenated with all other lists later. \n",
    "    \n",
    "    btc_price_list = df.values.tolist()\n",
    "    \n",
    "    return btc_price_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "59ee38eb-e20c-42d8-8372-88cb0d35c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lineplot(x = 'Date', y = 'Mean sentiment of daily tweets', data = df_bitcoin).set(xlabel = 'Date', ylabel = 'Mean sentiment over select tweets of given day')\n",
    "\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff70494e-288d-459f-a6c2-a42a0b73ed99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter a keyword or phrase to focus your search:  bitcoin \n",
      "Please enter the number of Tweets you would like to analyse per day:  100\n",
      "How many of the last days would you like to analyse?:   150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping and sentiment analysis complete!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "df_bitcoin  created!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Date</th>\n",
       "      <th>Mean reply count</th>\n",
       "      <th>Mean retweet count</th>\n",
       "      <th>Mean like count</th>\n",
       "      <th>Mean sentiment of daily tweets</th>\n",
       "      <th>Mean subjectivity of daily tweets</th>\n",
       "      <th>Adjusted closing asset price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bitcoin</td>\n",
       "      <td>2022-07-29</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.77</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.100597</td>\n",
       "      <td>0.278271</td>\n",
       "      <td>19552.054688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bitcoin</td>\n",
       "      <td>2022-07-30</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.45</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.197435</td>\n",
       "      <td>0.316977</td>\n",
       "      <td>19430.144531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bitcoin</td>\n",
       "      <td>2022-07-31</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.039936</td>\n",
       "      <td>0.288098</td>\n",
       "      <td>19178.634766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bitcoin</td>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.117555</td>\n",
       "      <td>0.304834</td>\n",
       "      <td>19024.222656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bitcoin</td>\n",
       "      <td>2022-08-02</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.117630</td>\n",
       "      <td>0.316604</td>\n",
       "      <td>18922.662109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>bitcoin</td>\n",
       "      <td>2022-12-21</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.052238</td>\n",
       "      <td>0.270928</td>\n",
       "      <td>13925.944336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>bitcoin</td>\n",
       "      <td>2022-12-22</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.162072</td>\n",
       "      <td>0.341690</td>\n",
       "      <td>13981.368164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>bitcoin</td>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.62</td>\n",
       "      <td>12.79</td>\n",
       "      <td>0.224089</td>\n",
       "      <td>0.351902</td>\n",
       "      <td>13935.910156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>bitcoin</td>\n",
       "      <td>2022-12-24</td>\n",
       "      <td>2.56</td>\n",
       "      <td>4.58</td>\n",
       "      <td>15.37</td>\n",
       "      <td>-0.002429</td>\n",
       "      <td>0.327548</td>\n",
       "      <td>13978.060547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>bitcoin</td>\n",
       "      <td>2022-12-25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.068935</td>\n",
       "      <td>0.324053</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Keyword        Date  Mean reply count  Mean retweet count  \\\n",
       "0    bitcoin   2022-07-29              0.87                0.77   \n",
       "1    bitcoin   2022-07-30              1.42                1.45   \n",
       "2    bitcoin   2022-07-31              1.00                0.31   \n",
       "3    bitcoin   2022-08-01              0.74                0.36   \n",
       "4    bitcoin   2022-08-02              0.80                0.20   \n",
       "..        ...         ...               ...                 ...   \n",
       "145  bitcoin   2022-12-21              0.25                0.02   \n",
       "146  bitcoin   2022-12-22              0.94                0.75   \n",
       "147  bitcoin   2022-12-23              1.81                1.62   \n",
       "148  bitcoin   2022-12-24              2.56                4.58   \n",
       "149  bitcoin   2022-12-25              0.20                0.20   \n",
       "\n",
       "     Mean like count  Mean sentiment of daily tweets  \\\n",
       "0               3.69                        0.100597   \n",
       "1               7.00                        0.197435   \n",
       "2               3.05                        0.039936   \n",
       "3               2.69                        0.117555   \n",
       "4               2.91                        0.117630   \n",
       "..               ...                             ...   \n",
       "145             1.13                        0.052238   \n",
       "146             3.75                        0.162072   \n",
       "147            12.79                        0.224089   \n",
       "148            15.37                       -0.002429   \n",
       "149             0.91                        0.068935   \n",
       "\n",
       "     Mean subjectivity of daily tweets  Adjusted closing asset price  \n",
       "0                             0.278271                  19552.054688  \n",
       "1                             0.316977                  19430.144531  \n",
       "2                             0.288098                  19178.634766  \n",
       "3                             0.304834                  19024.222656  \n",
       "4                             0.316604                  18922.662109  \n",
       "..                                 ...                           ...  \n",
       "145                           0.270928                  13925.944336  \n",
       "146                           0.341690                  13981.368164  \n",
       "147                           0.351902                  13935.910156  \n",
       "148                           0.327548                  13978.060547  \n",
       "149                           0.324053                           NaN  \n",
       "\n",
       "[150 rows x 8 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_scrape_enrich_aggregate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31a870-d568-45e8-ba49-d92de356e9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d23670-99b7-4f3f-81bf-bf8f9430a3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
